{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics in Regression using Count Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression analysis is a set of frameworks to identify the relationships among variables. It estimates the value of a variables, called a dependent variable, based on one or more variables, called independent variables (or explanatory variables). In regression analysis, one can find out which of those independent variables does indeed affect on the dependent variable.  Because of its easy of use and intuitive result, it has been widely applied in various fields and many techniques for different situation have been developed.\n",
    "\n",
    "As various statistical softwares and laguages have been introduced, it became very easy to perform regression analysis. If anyone has data and a model, one can do a regression analysis and interpret the result using programs such as SAS, SPSS, Stata, and R. These programs show various statistics and relationships between dependent variable and covariates as a summary of regression for reseachers then to interpret the result.\n",
    "\n",
    "![Example_of_regression_using count data](https://raw.githubusercontent.com/PyTrans/Statistics/master/regression_summary.png)\n",
    "\n",
    "This notebook is for those who wondering how a series of statistics provided by those packages are calculated as a result of regression analysis using count data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "The list of statistics to see in this notebook is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients\n",
    "#### Estimate\n",
    ": The regression coefficients are calculated by maximum likelihood estimation. Different regression methods have different models based on their own assumptions. Although likelihood function depends on the model, the principle of the estimation method is the same regardless of the model. The python package of scipy provides various methods to maximize likelihood function in regression estimation.[(scipy.optimize)](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html)\n",
    "\n",
    "#### Std. Error\n",
    ": Each of the model parameters has its own standard error, which is the estimated standard deviation of the error in estimating it. Note that asymptotic standard errors for the coefficients can be estimated using inverse of the Fisher information matrix which the negative of the second derivative (the Hessian matrix) of the log-likelihood (http://home.cc.umanitoba.ca/~godwinrt/7010/poissonregression.pdf).\n",
    "$$I(\\beta) = - \\frac{\\partial^2}{\\partial \\beta^2} LL(\\beta) = -H(\\beta)$$\n",
    "\n",
    "where, $I(\\beta)$ is Fisher information matrix [(Wiki)](https://en.wikipedia.org/wiki/Fisher_information)\n",
    "\n",
    "$LL(\\beta)$ is the log-likelihood function\n",
    "\n",
    "$H(\\beta)$ is the Hessian [(Wiki)](https://en.wikipedia.org/wiki/Hessian_matrix)\n",
    "#### z value\n",
    ": z-values of individual variables are computed as the test statistic whose null hypothesis is the corrseponding regression coefficient is zero.\n",
    "$$Z = \\frac{\\hat{\\beta} - 0}{\\hat{\\sigma}_{\\hat{\\beta}}}$$\n",
    "where, $\\hat{\\beta}$ is an estimated coefficient and $\\hat{\\sigma}_{\\hat{\\beta}}$ is standard error of the variable.\n",
    "(http://logisticregressionanalysis.com/1577-what-are-z-values-in-logistic-regression/)\n",
    "#### Pr(>|z|)\n",
    ": p-value corresponding to the z value. [(reliawiki)](http://reliawiki.org/index.php/Simple_Linear_Regression_Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deviance\n",
    "#### Null Deviance\n",
    ": How well the y variable is predicted by a model that includes only the intercept.\n",
    "\n",
    "$= -2\\left\\{LL(null) - LL(sat)\\right\\}$ on $df = df(sat) - df(null)$\n",
    "\n",
    "#### Residual deviance\n",
    "$= -2\\left\\{LL(reg) - LL(sat)\\right\\}$ on $df = df(sat) - df(reg)$\n",
    "\n",
    "where null model includes only the intercept, sat(saturated) model,the opposite of the null model, includes all parameters except for intercept, and regression model includes all the paramters as well as the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness-of-fit\n",
    "#### Deviance Residual\n",
    "A poorly fitting point will make a large contribution to the deviance, meaning absolute value of the deviance residual will become large [(refer)](http://wwwf.imperial.ac.uk/~bm508/teaching/AppStats/Lecture5.pdf).\n",
    "\n",
    "$$d_i = sgn\\left(y_i - exp(X_i \\hat{\\beta})\\right) \\sqrt{2\\left\\{y_i log\\left(\\frac{y_i}{exp(X_i \\hat{\\beta})}\\right)-\\left(y_i - exp(X_i \\hat{\\beta})\\right)\\right\\}}$$\n",
    "\n",
    "#### AIC\n",
    "The Akaike information criterion (AIC) is an estimator of the relative quality of statistical models for a given set of data [(Wiki)](https://en.wikipedia.org/wiki/Akaike_information_criterion)\n",
    "\n",
    "$$AIC = 2k - 2 ln(\\hat{LL})$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
