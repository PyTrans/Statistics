{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Data Analysis - Poisson Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poisson regression is one of the most popular methods to model count data which non-negative integer values in terms of its simplicity and ease of estimate. In the transportation studies, it has been applied to various traffic event datasets such as accident occurrence and vehicles in a queue. One assumption of the Poission regression is that the mean of the count data equals to its variance because of the nature of the Poission distribution. In other words, it is inappropriate to adopt the Poisson regression for over- or under-dispersed count datasets. Researchers have developed alternative methods to overcome this issue. Here is a review paper of the statistical alternatives for count data [(Lord, D., & Mannering, F. (2010))](https://www.researchgate.net/profile/Fred_Mannering/publication/222659783_The_Statistical_Analysis_of_Crash-Frequency_Data_A_Review_and_Assessment_of_Methodological_Alternatives/links/53d2b0f80cf228d363e950a4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(equations retrieved by [Washington, S. P., Karlaftis, M. G., & Mannering, F. (2010)](https://www.taylorfrancis.com/books/9781420082869))\n",
    "\n",
    "**The probability function of the Poisson regression** ($y$ occurrences in a unit time given occurrence rate $\\lambda$) is given by\n",
    "$$Pr(y_{i}) = \\frac{\\lambda_{i}^{y_i} e^{-\\lambda_i}}{y_i !}$$\n",
    "\n",
    "where $y_i$ is the number of events at data point $i$ and $\\lambda_i$ is occurrence rate, which equals to the expected number of events.\n",
    "\n",
    "The Poisson regression assumes 1) the values of the dependent variable follow a Poission distribution and a 2) log-linear model. Specifically, $ln(\\lambda_i)$ (logarithm of expected value) can be modeled by $\\beta X_i$ (a linear combination of coefficient parameters),\n",
    "\n",
    "$$ln(\\lambda_i) = \\beta X_i \\ or \\ \\lambda_i = e^{\\beta X_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(The likelihood function of Poisson regression)** Under the assumption of the independence of observations, the likelihood function equals to the product of the probability mass function (discrete case). Therefore, likelihood function of a Poisson regression is:\n",
    "\n",
    "$$L(\\beta) = \\frac{\\lambda_{1}^{y_1} e^{-\\lambda_1}}{y_1!} \\cdot \\frac{\\lambda_{2}^{y_2} e^{-\\lambda_2}}{y_2!} \\cdot ... \\cdot \\frac{\\lambda_{n}^{y_n} e^{-\\lambda_n}}{y_n!} = \\prod^{n}_{i=1} \\frac{\\lambda^{y_i}_{i} e^{-\\lambda_i}}{y_i !}$$\n",
    "where $\\lambda_i = e^{\\beta X_i}$\n",
    "\n",
    "For simpler estimation, we take the logarithm to the likelihood function,\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "LL(\\beta) &= ln \\left( \\prod^{n}_{i=1} \\frac{\\lambda^{y_i}_{i} e^{-\\lambda_i}}{y_i !} \\right) \\\\ &= \\sum^{n}_{i=1} ln \\left( \\frac{\\lambda^{y_i}_{i} e^{-\\lambda_i}}{y_i !} \\right) \\\\ &= \\sum^{n}_{i=1} \\left[ ln \\left( \\lambda^{y_i}_{i} \\right)+ ln \\left( e^{-\\lambda_i} \\right) - ln \\left( y_i ! \\right) \\right] \\\\ &= \\sum^{n}_{i=1} \\left[ y_i ln(\\lambda_i) - \\lambda_i - ln \\left( y_i ! \\right)  \\right] \\\\ &= \\sum^{n}_{i=1} \\left[ y_i \\beta X_i - e^{\\beta X_i} - ln \\left( y_i ! \\right) \\right]\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "**(Maximum likelihood estimation(MLE))** By maximizing the log-likelihood function (LL), we can find the estimated parameter values. In this project, we are going to minimize negative log-likelihood function using scipy package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook models a dataset of accidents on HOV facilities in the Southern California from 2014 to 2017 which provided by PeMS based on the Poisson regression. You can refer to the following notebook for description of dataset [(here)](). It turns out that the Poission distribution is not the best option to model the dataset, but this notebook intends to show a series of steps to find the solution of the Poisson regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the verification purpose, the result of the Poisson model implemented in R is attached as below,\n",
    "\n",
    "![Result of Poisson regression](https://raw.githubusercontent.com/PyTrans/Statistics/master/Images/poisson_regression_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats  \n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import autograd as ag\n",
    "import autograd.numpy as np\n",
    "import autograd.scipy as sp\n",
    "import functools\n",
    "import scipy.optimize\n",
    "import patsy\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aic(y, X, beta):\n",
    "    '''\n",
    "    Method for calculating Akaike information criterion(AIC) value\n",
    "    AIC = 2k - 2ln(L)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : numpy.array\n",
    "        dependent variable\n",
    "        \n",
    "    X : numpy.array\n",
    "        explanatory variables\n",
    "        \n",
    "    beta : numpy.array\n",
    "            regression coefficients\n",
    "            \n",
    "    Return\n",
    "    ------\n",
    "    float\n",
    "        AIC value\n",
    "            \n",
    "    '''\n",
    "    return 2*len(beta) + 2*get_poisson_neg_ll(y, X, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poisson_neg_ll(y, X, beta):\n",
    "    '''\n",
    "    Method for calculating a negative log-likelihood function of a Poisson model\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : numpy.array\n",
    "        dependent variable\n",
    "        \n",
    "    X : numpy.array\n",
    "        explanatory variables\n",
    "        \n",
    "    beta : numpy.array\n",
    "            regression coefficients\n",
    "            \n",
    "    Return\n",
    "    ------\n",
    "    float\n",
    "        negative log-likelihood value with current betas\n",
    "    '''\n",
    "    mu = np.exp(np.dot(X, beta))\n",
    "    ll = np.sum( y*np.log(mu) - mu - np.log(scipy.special.factorial(y)))\n",
    "    neg_ll = -ll\n",
    "    return neg_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Dataset\n",
    "\n",
    "You can also access the example dataset through PyTrans Github page [(GitHub)](https://github.com/PyTrans/Statistics/blob/master/Count-HOV_Accidents_SoCal.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'https://raw.githubusercontent.com/PyTrans/Statistics/master/Count-HOV_Accidents_SoCal.csv'\n",
    "hov_acci_socal = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model\n",
    "We assume that accidents frequency on the HOV facilities are correlated to the geometry of the highway segment. The geometry information of interest is:\n",
    "1. The number of HOV lanes\n",
    "2. Access type (0: continuous access, 1: limited access)\n",
    "3. Road width\n",
    "4. HOV lane width\n",
    "5. Inner shoulder width\n",
    "6. Outer shoulder width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Accidents~Lanes+Limited+RoadWidth+LaneWidth+InnerShoulderWidth+OuterShoulderWidth'\n",
    "\n",
    "y_patsy, X_patsy = patsy.dmatrices(model, hov_acci_socal)\n",
    "y = np.array(y_patsy).flatten()\n",
    "X = np.array(X_patsy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of variables\n",
    "varLength = len(model.split(sep='+'))\n",
    "\n",
    "# initial beta values \n",
    "# intercept - average of y values / other parameters - 0\n",
    "init_beta = np.array([np.log(np.mean(y))]+[0]*(varLength), dtype=float)\n",
    "\n",
    "# negative log-likelihood function\n",
    "neg_ll = functools.partial(get_poisson_neg_ll, y, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Option 1](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) - BFGS (the quasi-Newton method of Broyden, Fletcher, Goldfarb, and Shanno)\n",
    "\n",
    "It uses the first derivatives only. BFGS has proven good performance even for non-smooth optimizations. The method of Scipy package requires Jacobian (gradient) of objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 29519.506881\n",
      "         Iterations: 16\n",
      "         Function evaluations: 26\n",
      "         Gradient evaluations: 25\n"
     ]
    }
   ],
   "source": [
    "jacobian = ag.jacobian(neg_ll)\n",
    "coefficients = scipy.optimize.minimize(\n",
    "            neg_ll,\n",
    "            init_beta,\n",
    "            method = 'BFGS',\n",
    "            jac = jacobian,\n",
    "            options={'gtol': 1e-6, 'disp': True}\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Option 2](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) - Nelder-Mead\n",
    "\n",
    "It uses the Simplex algorithm. This algorithm has been successful in many applications but other algorithms using the first and/or second derivatives information might be preferred for their better performances and robustness in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " final_simplex: (array([[ 3.07657182,  0.28562475,  0.14693085,  0.00452472, -0.10636925,\n",
      "        -0.03229843,  0.0285439 ],\n",
      "       [ 3.07664558,  0.28561917,  0.14693791,  0.00452494, -0.10637561,\n",
      "        -0.03229809,  0.02854258],\n",
      "       [ 3.07662252,  0.285608  ,  0.14693721,  0.00452491, -0.1063704 ,\n",
      "        -0.03229874,  0.02854043],\n",
      "       [ 3.07655053,  0.2856232 ,  0.14693554,  0.00452493, -0.10636862,\n",
      "        -0.03229791,  0.02854296],\n",
      "       [ 3.07660479,  0.28562886,  0.14694181,  0.00452449, -0.10637103,\n",
      "        -0.03229791,  0.02854222],\n",
      "       [ 3.0766112 ,  0.28563568,  0.1469263 ,  0.00452487, -0.10637257,\n",
      "        -0.03229799,  0.02854185],\n",
      "       [ 3.07651499,  0.28565578,  0.14693389,  0.00452471, -0.10636528,\n",
      "        -0.03229996,  0.02854185],\n",
      "       [ 3.07665359,  0.28564164,  0.14692357,  0.00452473, -0.10637495,\n",
      "        -0.03229937,  0.02854143]]), array([29519.50688145, 29519.50688149, 29519.50688169, 29519.50688184,\n",
      "       29519.50688185, 29519.50688194, 29519.50688196, 29519.50688227]))\n",
      "           fun: 29519.506881450863\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 1298\n",
      "           nit: 874\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([ 3.07657182,  0.28562475,  0.14693085,  0.00452472, -0.10636925,\n",
      "       -0.03229843,  0.0285439 ])\n"
     ]
    }
   ],
   "source": [
    "coefficients = scipy.optimize.minimize(\n",
    "            neg_ll,\n",
    "            init_beta,\n",
    "            method = 'Nelder-Mead',\n",
    "            )\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Option 3](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize)  - Newton-CG (Newton-Conjugate-Gradient) \n",
    "\n",
    "(retrieved from [HERE](http://folk.uio.no/inf3330/scripting/doc/python/SciPy/tutorial/old/node13.html))\n",
    "\n",
    "It requires the fewest function calls and is therefore often the fastest method to minimize functions of many variables. This method is based on fitting the function locally to a quadratic form:\n",
    "\n",
    "$$f(x) \\approx f(x_0)+\\bigtriangledown f(x_0) \\cdot (x-x_0)+\\frac{1}{2} (x-x_0)^T H(x_0) (x-x_0)$$\n",
    "\n",
    "The python method requires Jacobian and Hessian of the objective function as parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 29519.506881\n",
      "         Iterations: 25\n",
      "         Function evaluations: 43\n",
      "         Gradient evaluations: 67\n",
      "         Hessian evaluations: 25\n"
     ]
    }
   ],
   "source": [
    "hessian = ag.hessian(neg_ll)\n",
    "coefficients = scipy.optimize.fmin_ncg(\n",
    "            neg_ll,\n",
    "            init_beta,\n",
    "            fprime= jacobian,\n",
    "            fhess= hessian,\n",
    "            avextol=1e-8\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(Intercept)', 'Lanes', 'Limited', 'RoadWidth', 'LaneWidth', 'InnerShoulderWidth', 'OuterShoulderWidth']\n"
     ]
    }
   ],
   "source": [
    "# Set variable names\n",
    "res_var_list = model.split(sep='~')[1].split(sep='+')\n",
    "res_var_list.insert(0, '(Intercept)')\n",
    "print(res_var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.07655315,  0.28562301,  0.14693696,  0.00452482, -0.10636758,\n",
       "       -0.03229991,  0.02854349])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The standard error of each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10245539 0.02306034 0.01205643 0.00042399 0.00825385 0.00168098\n",
      " 0.00286214]\n"
     ]
    }
   ],
   "source": [
    "stdErr = np.diag(np.sqrt(np.linalg.inv(hessian(coefficients))))\n",
    "print(stdErr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The z value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30.02822034  12.38589694  12.18743438  10.672015   -12.88702277\n",
      " -19.21494519   9.97279589]\n"
     ]
    }
   ],
   "source": [
    "z_value = coefficients/stdErr\n",
    "print(z_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr(>|z|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "p_value = np.round(stats.norm.sf(abs(z_value)) * 2, 4)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The estimated y value\n",
    "\n",
    "$\\hat{Y}= exp(X\\beta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.27025413, 14.3328991 , 16.60152382, ..., 18.48847962,\n",
       "       13.41047072, 12.33316935])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = np.exp(np.dot(X, coefficients))\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deviance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null Deviance\n",
    "\n",
    "$=-2 \\left\\{ LL(null) - LL(sat) \\right\\}$ on $df=df(sat)-df(null)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Deviance\n",
    "\n",
    "$=-2 \\left\\{ LL(reg) - LL(sat) \\right\\}$ on $df=df(sat)-df(reg)$\n",
    "\n",
    "where null model includes only the intercept, sat(saturated) model,the opposite of the null model, includes all parameters except for intercept, and regression model includes all the paramters as well as the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log-likelihood of the regression model - $LL(reg)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ll(regression)\n",
    "ll_reg = - neg_ll(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log-likelihood of the null model - $LL(null)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ll(null)\n",
    "model_null = 'Accidents~1'\n",
    "y_patsy, X_patsy = patsy.dmatrices(model_null, hov_acci_socal)\n",
    "y_null = np.array(y_patsy).flatten()\n",
    "X_null = np.array(X_patsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_beta = [0]\n",
    "neg_ll = functools.partial(get_poisson_neg_ll, y_null, X_null)\n",
    "jacobian = ag.jacobian(neg_ll)\n",
    "hessian = ag.hessian(neg_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 30344.007608\n",
      "         Iterations: 6\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 15\n",
      "         Hessian evaluations: 6\n"
     ]
    }
   ],
   "source": [
    "intercept = scipy.optimize.fmin_ncg(\n",
    "        neg_ll,\n",
    "        init_beta,\n",
    "        fprime= jacobian,\n",
    "        fhess= hessian,\n",
    "        avextol=1e-8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_null = - neg_ll(intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log-likelihood of the saturated model - $LL(sat)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3683.997935533842\n"
     ]
    }
   ],
   "source": [
    "## ll(saturated)\n",
    "ll_sat_arr = y * np.log(y) - y - np.log(scipy.special.factorial(y))\n",
    "ll_sat_list = []\n",
    "for ls in ll_sat_arr:\n",
    "    if np.isnan(ls):\n",
    "        ll_sat_list.append(0)\n",
    "    else:\n",
    "        ll_sat_list.append(ls)\n",
    "ll_sat = np.sum(ll_sat_list)\n",
    "print(ll_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null deviance: 53320.019345869456\n",
      "Residual deviance: 51671.01789035642\n"
     ]
    }
   ],
   "source": [
    "print('Null deviance: {}'.format(-2 * (ll_null - np.sum(ll_sat))))\n",
    "print('Residual deviance: {}'.format(-2 * (ll_reg - np.sum(ll_sat))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness-of-fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deviance residuals\n",
    "\n",
    "$$d_i = sgn\\left(y_i - exp(X_i \\hat{\\beta})\\right) \\sqrt{2\\left\\{y_i log\\left(\\frac{y_i}{exp(X_i \\hat{\\beta})}\\right)-\\left(y_i - exp(X_i \\hat{\\beta})\\right)\\right\\}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.19626094709004\n",
      "-6.766227525778918\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.exp(np.dot(X, coefficients))\n",
    "\n",
    "residuals = y - y_hat\n",
    "\n",
    "rtTerm = []\n",
    "for i in range(len(y)):\n",
    "    rtTerm1 = y[i] * np.log(y[i] / y_hat[i])\n",
    "    rtTerm2 = y[i] - y_hat[i]\n",
    "    \n",
    "    if np.isnan(rtTerm1):\n",
    "        rtTerm.append(2*(0 - rtTerm2))\n",
    "    else:\n",
    "        rtTerm.append(2*(rtTerm1 - rtTerm2))\n",
    "rtTerm  = np.array(rtTerm)\n",
    "\n",
    "d = np.sign(residuals) * np.sqrt(rtTerm)\n",
    "print(d.max())\n",
    "print(d.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AICc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59053.0137614241"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_aic(y, X, coefficients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
